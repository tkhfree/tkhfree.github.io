<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Ian Tang">


    <meta name="subtitle" content="岁月不居时节如流">


    <meta name="description" content="好记性不如烂笔头">



<title>tensorflow一些函数 | 随笔</title>



    <link rel="icon" href="/image/The_Lion_King_32_32.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Tangkaifei&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Tangkaifei&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">tensorflow一些函数</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Ian Tang</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">九月 18, 2019&nbsp;&nbsp;0:21:52</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="相关函数理解"><a href="#相关函数理解" class="headerlink" title="相关函数理解"></a>相关函数理解</h1><h2 id="tf-constant"><a href="#tf-constant" class="headerlink" title="tf.constant"></a>tf.constant</h2><p>生成常量张量</p>
<pre><code>constant(
    value,
    dtype=None,
    shape=None,
    name=&#39;Const&#39;,
    verify_shape=False
)
</code></pre>
<p><strong>value 必选 常量数值或者list</strong> 输出张量的值<br><strong>dtype 可选 dtype</strong> 输出张量元素类型<br><strong>shape 可选 1维整形张量或者array</strong> 输出张量的维度<br><strong>name 可选 string</strong> 张量名称<br><strong>verify_shape 可选 boolean</strong><br>检测shape是否和value一致，若为false，不一致时，会用最后一个元素补全shape</p>
<hr>
<h2 id="tf-placeholder"><a href="#tf-placeholder" class="headerlink" title="tf.placeholder"></a>tf.placeholder</h2><p>占位符</p>
<pre><code>placeholder(
    dtype,
    shape=None,
    name=None
)
</code></pre>
<p>dtype 占位符类型<br>shape 占位符维度</p>
<hr>
<h2 id="tf-nn-bias-add"><a href="#tf-nn-bias-add" class="headerlink" title="tf.nn.bias_add"></a>tf.nn.bias_add</h2><p>将偏差项 bias 加到 value 上面，可以看做是 tf.add 的一个特例，其中 bias 必须是一维的，并且维度和 value 的最后一维相同，数据类型必须和 value 相同。</p>
<pre><code>bias_add(
    value,
    bias,
    data_format=None,
    name=None
)
</code></pre>
<p><strong>value 必选 张量</strong> 	数据类型为 float, double, int64, int32, uint8, int16, int8, complex64, or complex128<br><strong>bias 必选 1维张量</strong>	维度必须和value最后一维维度相等<br><strong>data_format 可选 string</strong> 	数据格式，支持 ‘ NHWC ‘ 和 ‘ NCHW ‘<br><strong>name 可选 string</strong> 运算名称</p>
<pre><code>import tensorflow as tf
import numpy as np

a = tf.constant([[1.0, 2.0],[1.0, 2.0],[1.0, 2.0]])
b = tf.constant([2.0,1.0])
c = tf.constant([1.0])
sess = tf.Session()
print (sess.run(tf.nn.bias_add(a, b)))
#print (sess.run(tf.nn.bias_add(a,c))) error
print (&quot;##################################&quot;)
print (sess.run(tf.add(a, b)))
print (&quot;##################################&quot;)
print (sess.run(tf.add(a, c)))

运算结果：
[[3. 3.]
 [3. 3.]
 [3. 3.]]
##################################
[[3. 3.]
 [3. 3.]
 [3. 3.]]
##################################
[[2. 3.]
 [2. 3.]
 [2. 3.]]
</code></pre>
<hr>
<p>##tf.reduce_mean<br>计算张量 input_tensor 平均值</p>
<pre><code>reduce_mean(
    input_tensor,
    axis=None,
    keep_dims=False,
    name=None,
    reduction_indices=None
)
</code></pre>
<p><strong>input_tensor 必选 张量</strong> 输入带求平均值的张量<br><strong>axis 可选 	None、0、1</strong> None：全局求平均值；0：求每一列平均值；1：求每一行平均值<br><strong>keep_dims 可选 boolean</strong> 保留原来的维度<br><strong>name 可选 string</strong> 运算名字<br><strong>reduction_indices 可选 none</strong> 和axis等价，弃用、</p>
<hr>
<h2 id="tf-squared-difference"><a href="#tf-squared-difference" class="headerlink" title="tf.squared_difference"></a>tf.squared_difference</h2><p>计算张量 x、y 对应元素差平方</p>
<pre><code>squared_difference(
    x,
    y,
    name=None
)
</code></pre>
<hr>
<h2 id="tf-square"><a href="#tf-square" class="headerlink" title="tf.square"></a>tf.square</h2><p>计算张量对应元素平方</p>
<pre><code>square(
    x,
    name=None
)
</code></pre>
<hr>
<h2 id="tf-nn-conv2d-卷积核"><a href="#tf-nn-conv2d-卷积核" class="headerlink" title="tf.nn.conv2d 卷积核"></a>tf.nn.conv2d 卷积核</h2><pre><code>conv2d(
    input,
    filter,
    strides,
    padding,
    use_cudnn_on_gpu=True,
    data_format=&#39;NHWC&#39;,
    name=None
)
</code></pre>
<p><strong>input 必选 tensor</strong> 是一个四维的tensor，即[batch, in_height, in_width, in_channels]（若input是图像，[训练时一个batch的图片数量，图片高度，图片宽度，图像通道数]）</p>
<p><strong>filter 必选 tensor</strong> 是一个四维的tensor，即[filter_height, filter_width, in_channels, out_channels]（若input是图像，[卷积核的高度，宽度，图像通道数，卷积核的个数]）</p>
<p><strong>strides 必选 列表</strong> 长度为4的list，卷积时候在input上每一维的步长，一般strides[0]&#x3D; striders[3]&#x3D;1(strides[0]指向输入batch数量，strides[3]映射通道数，都为1就是指一幅一幅和一个通道一个通道卷积)</p>
<p><strong>padding 必选 string</strong><br>只能为“VALID”,”SAME”之一，决定了卷积方式，”VALID”：丢弃&#x2F;“SAME”：补全。</p>
<p>use_cudnn_on_gpu&#x3D;True,<br>data_format&#x3D;’NHWC’,<br>name&#x3D;None<br>都是可选的，分别是使用GPU加速，数据格式，运算名称</p>
<pre><code>import tensorflow as tf
a = tf.constant([1,1,1,0,0,0,1,1,1,0,0,0,1,1,1,0,0,1,1,0,0,1,1,0,0],dtype = tf.float32, shape = [1, 5, 5, 1])
b = tf.constant([1,0,1,0,1,0,1,0,1],dtype = tf.float32, shape = [1, 3, 3, 1])
c = tf.nn.conv2d(a,b,strides=[1, 2, 2, 1],padding=&#39;VALID&#39;)
d = tf.nn.conv2d(a,b,strides=[1, 2, 2, 1],padding=&#39;SAME&#39;)
with tf.Session() as sess:
    print (&quot;c shape:&quot;)
    print (c.shape)
    print (&quot;c value:&quot;)
    print (sess.run(c))
    print (&quot;d shape:&quot;)
    print (d.shape)
    print (&quot;d value:&quot;)
    print (sess.run(d))

cd /home/ubuntu;
python conv2d.py

c shape:
(1, 2, 2, 1)
c value:
[[[[4.]
   [4.]]

  [[2.]
   [4.]]]]
d shape:
(1, 3, 3, 1)
d value:
[[[[2.]
   [3.]
   [1.]]

  [[1.]
   [4.]
   [3.]]

  [[0.]
   [2.]
   [1.]]]]
</code></pre>
<hr>
<h2 id="tf-nn-relu-激活函数"><a href="#tf-nn-relu-激活函数" class="headerlink" title="tf.nn.relu 激活函数"></a>tf.nn.relu 激活函数</h2><pre><code>relu(
    features,
    name=None
)
</code></pre>
<p><strong>feature 必选 tensor</strong> 以下类型float32，float64，int32，int64，unit8，int16，int8，uint16，half<br><strong>name 可选 string</strong> 运算名称</p>
<pre><code>import tensorflow as tf

a = tf.constant([1,-3,0,6,-2,0,3])
b = tf.nn.relu(a)

with Session() as sess:
    print(sess.run(b))
    
执行结果：
[1,0,0,6,0,0,3]
</code></pre>
<hr>
<h2 id="tf-nn-max-pool-池化层"><a href="#tf-nn-max-pool-池化层" class="headerlink" title="tf.nn.max_pool 池化层"></a>tf.nn.max_pool 池化层</h2><pre><code>max_pool(
    value,
    ksize,
    strides,
    padding,
    data_format=&#39;NHWC&#39;,
    name=None
)
</code></pre>
<p><strong>value 必选 tensor</strong> 4维的张量，即[batch，height，width，channels]，数据类型为tf.float32<br><strong>ksize 必选 列表</strong> 池化窗口的大小，长度为4的list，一般是<a href="%E7%B1%BB%E4%BC%BC%E4%BA%8Econv2d%E5%87%BD%E6%95%B0%E7%9A%84strides%E5%8F%82%E6%95%B0">1,height,width,1</a>，第一个维度和最后一个维度是batch和channels的池化窗口大小<br><strong>strides 必选 列表</strong> 池化窗口在每一维的步长，一般strides[0]&#x3D;strides[3]&#x3D;1<br><strong>padding 必选 string</strong></p>
<p>只能为”VALID”,”SAME”中的一个，这个值决定了不同的池化方式。VALID是丢弃，SAME是补全。</p>
<p><strong>data_format 可选 string</strong> 默认”NHWC”</p>
<p><strong>name 可选 string</strong> 运算名称</p>
<pre><code>import tensorflow as tf

a = tf.constant([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20],[21,22,23,24,25]],dtype=tf.float32,shape=[1,5,5,1])
b = tf.nn.max_pool(a,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding=&#39;VALID&#39;)
c = tf.nn.max_pool(a,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding=&#39;SAME&#39;)
with tf.Session() as sess:
    print (&quot;b shape:&quot;)
    print (b.shape)
    print (&quot;b value:&quot;)
    print (sess.run(b))
    print (&quot;c shape:&quot;)
    print (c.shape)
    print (&quot;c value:&quot;)
    print (sess.run(c))
    
    运算结果：
b shape:
(1, 2, 2, 1)
b value:
[[[[ 7.]
   [ 9.]]

  [[17.]
   [19.]]]]
c shape:
(1, 3, 3, 1)
c value:
[[[[ 7.]
   [ 9.]
   [10.]]

  [[17.]
   [19.]
   [20.]]

  [[22.]
   [24.]
   [25.]]]]
   池化窗口偶数，value奇数尺寸，valid直接舍弃最后一列，same最后补0.
   ksize与最后计算的尺寸没有关系，只是池化的范围
</code></pre>
<hr>
<h2 id="tf-nn-dropout"><a href="#tf-nn-dropout" class="headerlink" title="tf.nn.dropout"></a>tf.nn.dropout</h2><p>防止过拟合，对于神经网络单元，按照一定的概率将其暂时随机的从网络中丢弃。对于随机梯度下降来说，由于是随机丢弃，所以每一个mini-batch都在训练不同的网络。</p>
<pre><code>dropout(
    x,
    keep_prob,
    noise_shape=None,
    seed=None,
    name=None
)
</code></pre>
<p><strong>x 必选 tensor</strong> 输出元素是x中的元素，数值变为1&#x2F;keep_prob，否则为0<br><strong>keep_prob 必选 scalar tensor</strong> dropout的概率，一般是占位符。设置神经元被选中的概率,在初始化时keep_prob是一个占位符,  keep_prob &#x3D; tf.placeholder(tf.float32) 。tensorflow在run时设置keep_prob具体的值，例如keep_prob: 0.5<br><strong>nosie_shape 可选 tensor</strong> 默认情况下，每个元素是否dropout是相互独立。如果制定noise_shape，若noise_shape[i]&#x3D;shape(x)[i],该维度的元素是否dropout是相互独立。若noise_shape[i] !&#x3D; shape(x)[i] 该维度元素是否 dropout 不相互独立，要么一起 dropout 要么一起保留。看shape和value_shape行列数是否一致，行一致，则行向量要么为0，要么都dropout；列同理。<br><strong>seed 可选 数值</strong> 如果指定该值，每次dropout结果相同<br><strong>name 可选 string</strong> 运算名称</p>
<pre><code>import tensorflow as tf

a = tf.constant([1,2,3,4,5,6],shape=[2,3],dtype=tf.float32)
b = tf.placeholder(tf.float32)
c = tf.nn.dropout(a,b,[2,1],1)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print (sess.run(c,feed_dict=&#123;b:0.75&#125;))
    
运算结果：
[[ 0.          0.          0.        ]
 [ 5.33333349  6.66666651  8.        ]]
</code></pre>
<hr>
<h2 id="tf-nn-sigmoid-cross-entropy-with-logits"><a href="#tf-nn-sigmoid-cross-entropy-with-logits" class="headerlink" title="tf.nn.sigmoid_cross_entropy_with_logits"></a>tf.nn.sigmoid_cross_entropy_with_logits</h2><p>这个函数的作用是计算经sigmoid 函数激活之后的交叉熵 &#x2F;labels和logits之间的交叉熵（cross entropy）<br>&#x2F;先对 logits 通过 sigmoid 计算，再计算交叉熵</p>
<pre><code>sigmoid_cross_entropy_with_logits(
    _sentinel=None,
    labels=None,
    logits=None,
    name=None
)
</code></pre>
<p>为了描述简洁，我们规定 x &#x3D; logits，z &#x3D; targets，那么 Logistic 损失值为：</p>
<pre><code>max(x,0)−x∗z+log(1+exp(−abs(x)))
</code></pre>
<p>log以e为底的对数，exp是以e为底的指数函数<br>logits 和 targets 必须有相同的数据类型和数据维度。<br>它适用于每个类别相互独立但互不排斥的情况,在一张图片中，同时包含多个分类目标（大象和狗），那么就可以使用这个函数。<br> <strong>_sentinel 可选 none</strong> 一般情况下没有使用的参数<br><strong>labels 可选 tensor</strong> type，shape与logits相同。分类标签，所不同的是，这个label是分类的概率，比如说[0.2,0.3,0.5]，labels的每一行必须是一个概率分布。<br><strong>logits 可选 tensor</strong> type是float32或者float64<br><strong>name 可选 string</strong> 运算名称</p>
<pre><code>import tensorflow as tf
x = tf.constant([1,2,3,4,5,6,7],dtype=tf.float64)
y = tf.constant([1,1,1,0,0,1,0],dtype=tf.float64)
loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = y,logits = x)
with tf.Session() as sess:
    print (sess.run(loss))
    
运算结果：
 [3.13261688e-01 1.26928011e-01 4.85873516e-02 4.01814993e+00
 5.00671535e+00 2.47568514e-03 7.00091147e+00]
</code></pre>
<p>关于交叉熵的理解，参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/wenzishou/article/details/77618992">交叉熵为何能做损失函数</a></p>
<hr>
<h2 id="tf-truncated-normal"><a href="#tf-truncated-normal" class="headerlink" title="tf.truncated_normal"></a>tf.truncated_normal</h2><p>产生截断正态分布随机数，取值范围为 [ mean - 2 * stddev, mean + 2 * stddev ]</p>
<pre><code>truncated_normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.float32,
    seed=None,
    name=None
)
</code></pre>
<p><strong>shape 必选 1维整形张量或array</strong> 输出张量的维度<br><strong>mean 可选 0维张量或数值</strong> 均值<br><strong>stddev 可选 0维张量或数值</strong> 标准差<br><strong>dtype 可选 dtype</strong> 输出类型<br><strong>seed 可选 数值</strong> 随机种子，若seed赋值，每次产生相同随机数<br><strong>name 可选 string</strong> 运算名称</p>
<pre><code>import tensorflow as tf
initial = tf.truncated_normal(shape=[3,3], mean=0, stddev=1)
print(tf.Session().run(initial))

输出结果：
产生一个取值范围 [ -2, 2 ] 的 3 * 3 矩阵
</code></pre>
<hr>
<h1 id="相关类理解"><a href="#相关类理解" class="headerlink" title="相关类理解"></a>相关类理解</h1><h2 id="tf-Variable"><a href="#tf-Variable" class="headerlink" title="tf.Variable"></a>tf.Variable</h2><p>维护图在执行过程中的状态信息，例如神经网络权重值的变化。</p>
<pre><code>__init__(
    initial_value=None,
    trainable=True,
    collections=None,
    validate_shape=True,
    caching_device=None,
    name=None,
    variable_def=None,
    dtype=None,
    expected_shape=None,
    import_scope=None
)
</code></pre>
<p><strong>initial_value 张量</strong> Variable 类的初始值，这个变量必须指定 shape 信息，否则后面 validate_shape 需设为 False<br><strong>trainable boolean</strong> 是否把变量添加到 collection GraphKeys.TRAINABLE_VARIABLES 中（collection 是一种全局存储，不受变量名生存空间影响，一处保存，到处可取）<br><strong>collections Graph collections</strong> 全局存储，默认是 GraphKeys.GLOBAL_VARIABLES<br><strong>validate_shape	Boolean</strong>	是否允许被未知维度的 initial_value 初始化<br><strong>caching_device	string</strong>	指明哪个 device 用来缓存变量<br><strong>name	string</strong>	变量名<br><strong>dtype	dtype</strong>	如果被设置，初始化的值就会按照这个类型初始化<br><strong>expected_shape	TensorShape</strong>	要是设置了，那么初始的值会是这种维度</p>
<pre><code>import tensorflow as tf
initial = tf.truncated_normal(shape=[3,3],mean=0,stddev=1)
W=tf.Variable(initial)
list = [[1.,1.],[2.,2.]]
X = tf.Variable(list,dtype=tf.float32)
init_op = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init_op)
    print (&quot;##################(1)################&quot;)
    print (sess.run(W))
    print (&quot;##################(2)################&quot;)
    print (sess.run(W[:2,:2]))
    op = W[:2,:2].assign(22.*tf.ones((2,2)))
    print (&quot;###################(3)###############&quot;)
    print (sess.run(op))
    print (&quot;###################(4)###############&quot;)
    print (W.eval(sess)) #computes and returns the value of this variable类似tf.Session.run(W)
    print (&quot;####################(5)##############&quot;)
    print (W.eval())  #Usage with the default session
    print (&quot;#####################(6)#############&quot;)
    print (W.dtype)
    print (sess.run(W.initial_value))
    print (sess.run(W.op))
    print (W.shape)
    print (&quot;###################(7)###############&quot;)
    print (sess.run(X))
</code></pre>
<hr>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Ian Tang</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://tkhfree.github.io/2019/09/18/tensorflow%E4%B8%80%E4%BA%9B%E5%87%BD%E6%95%B0/">https://tkhfree.github.io/2019/09/18/tensorflow%E4%B8%80%E4%BA%9B%E5%87%BD%E6%95%B0/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Tensorflow/"># Tensorflow</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2019/09/18/sqlaichemy%E5%AD%A6%E4%B9%A0/">sqlaichemy学习</a>
            
            
            <a class="next" rel="next" href="/2019/08/18/%E6%97%A5%E5%BF%97%E6%A6%82%E8%BF%B0/">web开发中日志代码概述</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Ian Tang | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>